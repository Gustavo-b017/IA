{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA-Ffwl-x_tv",
        "outputId": "6daf043b-bf4a-4528-e6c0-4e2dda096082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ml-100k could not be found. Do you want to download it? [Y/n] Y\n",
            "Trying to download dataset from https://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
            "Done! Dataset ml-100k has been saved to /root/.surprise_data/ml-100k\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Best RMSE: 0.9292234282327344\n",
            "Best params (RMSE): {'k': 20, 'min_k': 5, 'sim_options': {'name': 'pearson_baseline', 'min_support': 1, 'user_based': False}}\n",
            "Best MAE: 0.727974119514809\n",
            "Best params (MAE): {'k': 20, 'min_k': 5, 'sim_options': {'name': 'pearson_baseline', 'min_support': 1, 'user_based': False}}\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "RMSE: 0.9175\n",
            "MAE:  0.7188\n",
            "\n",
            "Top-10 recommendations for user 934:\n",
            "  MovieID=483  |  PredRating=4.656\n",
            "  MovieID=64  |  PredRating=4.523\n",
            "  MovieID=79  |  PredRating=4.517\n",
            "  MovieID=12  |  PredRating=4.441\n",
            "  MovieID=98  |  PredRating=4.429\n",
            "  MovieID=945  |  PredRating=4.421\n",
            "  MovieID=22  |  PredRating=4.413\n",
            "  MovieID=313  |  PredRating=4.390\n",
            "  MovieID=272  |  PredRating=4.367\n",
            "  MovieID=190  |  PredRating=4.362\n",
            "\n",
            "5-fold CV (refit with best params) -> RMSE mean: 0.9174 ± 0.0059, MAE mean: 0.7184 ± 0.0058\n"
          ]
        }
      ],
      "source": [
        "# knn_item_cf_surprise_gridsearch.py\n",
        "# Item-based KNN CF with Surprise + GridSearchCV on MovieLens 100K.\n",
        "\n",
        "from surprise import Dataset, KNNBaseline, KNNBasic, accuracy\n",
        "from surprise.model_selection import GridSearchCV, train_test_split, cross_validate\n",
        "from collections import defaultdict\n",
        "\n",
        "# 1) Load data (MovieLens 100k). First run will download it.\n",
        "data = Dataset.load_builtin(\"ml-100k\")\n",
        "\n",
        "# 2) Quick holdout for final evaluation after model selection\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3) Hyperparameter search (3-fold CV) on the TRAIN portion only\n",
        "#    We search only ITEM-BASED (user_based=False). Try both cosine and pearson_baseline.\n",
        "param_grid = {\n",
        "    \"k\": [10,20,30,40],\n",
        "    \"min_k\": [1, 3, 5],\n",
        "    \"sim_options\": {\n",
        "        \"name\": [\"cosine\", \"pearson_baseline\"],\n",
        "        \"min_support\": [1, 5],\n",
        "        \"user_based\": [False],  # False -> item-based\n",
        "    },\n",
        "    # Optional (only used by KNNBaseline): baseline estimates (ALS) for debiasing\n",
        "    # \"bsl_options\": [\n",
        "    #     {\"method\": \"als\", \"n_epochs\": 10, \"reg_u\": 15, \"reg_i\": 10},\n",
        "    #     {\"method\": \"als\", \"n_epochs\": 15, \"reg_u\": 12, \"reg_i\": 8},\n",
        "    # ]\n",
        "}\n",
        "\n",
        "# You can switch to KNNBasic below if you want a simpler algorithm:\n",
        "AlgoClass = KNNBaseline  # or KNNBasic\n",
        "\n",
        "gs = GridSearchCV(\n",
        "    AlgoClass,\n",
        "    param_grid,\n",
        "    measures=[\"rmse\", \"mae\"],\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    joblib_verbose=0,\n",
        "    refit=True,  # refit on full CV training folds using best params (rmse by default)\n",
        ")\n",
        "\n",
        "# Fit grid search on the original full dataset (it will internally do CV)\n",
        "gs.fit(data)\n",
        "\n",
        "print(\"Best RMSE:\", gs.best_score[\"rmse\"])\n",
        "print(\"Best params (RMSE):\", gs.best_params[\"rmse\"])\n",
        "print(\"Best MAE:\", gs.best_score[\"mae\"])\n",
        "print(\"Best params (MAE):\", gs.best_params[\"mae\"])\n",
        "\n",
        "# 4) Train the best model on the *trainset* (from the holdout split)\n",
        "best_params = gs.best_params[\"rmse\"]\n",
        "algo = AlgoClass(**best_params)\n",
        "algo.fit(trainset)\n",
        "\n",
        "# 5) Evaluate on testset (holdout)\n",
        "predictions = algo.test(testset)\n",
        "rmse = accuracy.rmse(predictions, verbose=True)\n",
        "mae = accuracy.mae(predictions, verbose=True)\n",
        "\n",
        "# 6) Produce Top-N recommendations for a given user (by raw user id, as string/int)\n",
        "def get_top_n_for_user(algo, trainset, raw_uid, n=10):\n",
        "    \"\"\"Return top-n (item_raw_id, estimated_rating) for items the user hasn't rated.\"\"\"\n",
        "    # Surprise uses inner ids internally; map raw -> inner\n",
        "    try:\n",
        "        inner_uid = trainset.to_inner_uid(str(raw_uid))\n",
        "    except ValueError:\n",
        "        raise ValueError(f\"User {raw_uid} not found in training set.\")\n",
        "\n",
        "    # Items the user has already interacted with\n",
        "    items_rated_by_user = set(j for (j, _) in trainset.ur[inner_uid])\n",
        "\n",
        "    # Iterate over all items; score only those not yet rated\n",
        "    candidates = []\n",
        "    for inner_iid in range(trainset.n_items):\n",
        "        if inner_iid in items_rated_by_user:\n",
        "            continue\n",
        "        raw_iid = trainset.to_raw_iid(inner_iid)\n",
        "        # Predict on-the-fly; Surprise takes (raw_uid, raw_iid)\n",
        "        est = algo.predict(str(raw_uid), raw_iid, verbose=False).est\n",
        "        candidates.append((raw_iid, est))\n",
        "\n",
        "    # Sort by estimated rating descending and return top-n\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return candidates[:n]\n",
        "\n",
        "# Example: recommendations for user \"196\" (a common user id in ML-100K)\n",
        "try:\n",
        "    topn = get_top_n_for_user(algo, trainset, raw_uid=\"934\", n=10)\n",
        "    print(\"\\nTop-10 recommendations for user 934:\")\n",
        "    for iid, est in topn:\n",
        "        print(f\"  MovieID={iid}  |  PredRating={est:.3f}\")\n",
        "except ValueError as e:\n",
        "    print(e)\n",
        "\n",
        "# 7) (Optional) Cross-validate the final AlgoClass with the found best_params on full data\n",
        "#    This shows more stable metrics across folds.\n",
        "final_algo = AlgoClass(**best_params)\n",
        "cv_results = cross_validate(final_algo, data, measures=[\"rmse\", \"mae\"], cv=5, verbose=False, n_jobs=-1)\n",
        "print(\n",
        "    \"\\n5-fold CV (refit with best params) -> RMSE mean: {:.4f} ± {:.4f}, MAE mean: {:.4f} ± {:.4f}\".format(\n",
        "        cv_results[\"test_rmse\"].mean(), cv_results[\"test_rmse\"].std(),\n",
        "        cv_results[\"test_mae\"].mean(), cv_results[\"test_mae\"].std()\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKm_v2rIyGUS",
        "outputId": "90b806cf-a390-47ce-95fe-0154c1a5488f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m133.1/154.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.16.2)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp312-cp312-linux_x86_64.whl size=2611305 sha256=238270df1f0dc15f78d46c6e12b282395edfe12f7757b2422f16acce841b6db9\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/fa/bc/739bc2cb1fbaab6061854e6cfbb81a0ae52c92a502a7fa454b\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y scikit-surprise surprise\n",
        "!pip install \"numpy<2\"\n",
        "!pip install --no-binary scikit-surprise scikit-surprise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9uSAnfLyO88",
        "outputId": "17881407-a900-4531-9792-90c5245432f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping scikit-surprise as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping surprise as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Collecting scikit-surprise\n",
            "  Using cached scikit_surprise-1.1.4-cp312-cp312-linux_x86_64.whl\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.16.2)\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# ---- CONFIG ----\n",
        "DATA_PATH = \"/root/.surprise_data/ml-100k/ml-100k\"   #  adjust if needed\n",
        "USER_ID = 934                                        # target user\n",
        "TOPN = 10                                            # how many recommendations to show\n",
        "\n",
        "# ---- LOAD MOVIELENS DATA ----\n",
        "ratings = pd.read_csv(\n",
        "    os.path.join(DATA_PATH, \"u.data\"),\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
        ")\n",
        "\n",
        "movies = pd.read_csv(\n",
        "    os.path.join(DATA_PATH, \"u.item\"),\n",
        "    sep=\"|\",\n",
        "    header=None,\n",
        "    encoding=\"ISO-8859-1\",\n",
        "    names=[\n",
        "        \"movie_id\", \"title\", \"release_date\", \"video_release_date\", \"imdb_url\",\n",
        "        \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\",\n",
        "        \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
        "        \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
        "    ],\n",
        "    usecols=[\"movie_id\", \"title\"]\n",
        ")\n",
        "\n",
        "# ---- USER 196'S EXISTING RATINGS ----\n",
        "user_ratings = (\n",
        "    ratings[ratings[\"user_id\"] == USER_ID]\n",
        "    .merge(movies, on=\"movie_id\", how=\"left\")\n",
        "    .sort_values(\"rating\", ascending=False)\n",
        ")\n",
        "print(f\"\\n🎬 User {USER_ID} has rated {len(user_ratings)} movies — top of their list:\")\n",
        "for _, row in user_ratings.head(20).iterrows():\n",
        "    print(f\"  ⭐ {row['title']}  |  Rating = {row['rating']}\")\n",
        "\n",
        "# ---- RECOMMENDATIONS FROM TRAINED KNN MODEL ----\n",
        "# assumes 'algo' (trained Surprise model) and 'trainset' already exist in memory\n",
        "def get_top_n_for_user(algo, trainset, raw_uid, n=10):\n",
        "    \"\"\"Return top-n (item_raw_id, estimated_rating) for items the user hasn't rated.\"\"\"\n",
        "    inner_uid = trainset.to_inner_uid(str(raw_uid))\n",
        "    items_rated_by_user = set(j for (j, _) in trainset.ur[inner_uid])\n",
        "    candidates = []\n",
        "    for inner_iid in range(trainset.n_items):\n",
        "        if inner_iid in items_rated_by_user:\n",
        "            continue\n",
        "        raw_iid = trainset.to_raw_iid(inner_iid)\n",
        "        est = algo.predict(str(raw_uid), raw_iid, verbose=False).est\n",
        "        candidates.append((int(raw_iid), float(est)))\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return candidates[:n]\n",
        "\n",
        "# Generate Top-N predictions\n",
        "topn = get_top_n_for_user(algo, trainset, raw_uid=USER_ID, n=TOPN)\n",
        "\n",
        "# Merge with movie titles\n",
        "topn_df = pd.DataFrame(topn, columns=[\"movie_id\", \"pred_rating\"]).merge(movies, on=\"movie_id\", how=\"left\")\n",
        "\n",
        "print(f\"\\n🍿 Top-{TOPN} recommendations for user {USER_ID} based on KNN predictions:\")\n",
        "for _, row in topn_df.iterrows():\n",
        "    print(f\"  🎥 {row['title']}  |  Predicted Rating = {row['pred_rating']:.3f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_UCBTHJzcqH",
        "outputId": "b3f35102-b6d1-42aa-b4a0-e9746c21f798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎬 User 934 has rated 174 movies — top of their list:\n",
            "  ⭐ Star Wars (1977)  |  Rating = 5\n",
            "  ⭐ Dave (1993)  |  Rating = 5\n",
            "  ⭐ Young Frankenstein (1974)  |  Rating = 5\n",
            "  ⭐ Dead Poets Society (1989)  |  Rating = 5\n",
            "  ⭐ Top Hat (1935)  |  Rating = 5\n",
            "  ⭐ Forrest Gump (1994)  |  Rating = 5\n",
            "  ⭐ Raiders of the Lost Ark (1981)  |  Rating = 5\n",
            "  ⭐ Annie Hall (1977)  |  Rating = 5\n",
            "  ⭐ Sunset Blvd. (1950)  |  Rating = 5\n",
            "  ⭐ Fantasia (1940)  |  Rating = 5\n",
            "  ⭐ Being There (1979)  |  Rating = 5\n",
            "  ⭐ Pulp Fiction (1994)  |  Rating = 5\n",
            "  ⭐ My Life as a Dog (Mitt liv som hund) (1985)  |  Rating = 5\n",
            "  ⭐ Amadeus (1984)  |  Rating = 5\n",
            "  ⭐ My Favorite Year (1982)  |  Rating = 5\n",
            "  ⭐ Mighty Aphrodite (1995)  |  Rating = 5\n",
            "  ⭐ Some Folks Call It a Sling Blade (1993)  |  Rating = 5\n",
            "  ⭐ Magnificent Seven, The (1954)  |  Rating = 5\n",
            "  ⭐ Get Shorty (1995)  |  Rating = 5\n",
            "  ⭐ Fried Green Tomatoes (1991)  |  Rating = 5\n",
            "\n",
            "🍿 Top-10 recommendations for user 934 based on KNN predictions:\n",
            "  🎥 Casablanca (1942)  |  Predicted Rating = 4.763\n",
            "  🎥 Charade (1963)  |  Predicted Rating = 4.682\n",
            "  🎥 Henry V (1989)  |  Predicted Rating = 4.641\n",
            "  🎥 Titanic (1997)  |  Predicted Rating = 4.597\n",
            "  🎥 Usual Suspects, The (1995)  |  Predicted Rating = 4.557\n",
            "  🎥 Wrong Trousers, The (1993)  |  Predicted Rating = 4.531\n",
            "  🎥 Braveheart (1995)  |  Predicted Rating = 4.527\n",
            "  🎥 Shawshank Redemption, The (1994)  |  Predicted Rating = 4.521\n",
            "  🎥 Lawrence of Arabia (1962)  |  Predicted Rating = 4.503\n",
            "  🎥 Close Shave, A (1995)  |  Predicted Rating = 4.442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jz1YWune3o-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}